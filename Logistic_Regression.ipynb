{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is used for classification, not regression.\n",
        "\n",
        "It predicts probabilities and uses the sigmoid function to output values between 0 and 1.\n",
        "\n",
        "Linear Regression predicts continuous values using a straight line.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n"
      ],
      "metadata": {
        "id": "PMTx5d0Bn9yP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "To map any real-valued input into the range (0, 1), representing probabilities.\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "Log loss (Binary Cross Entropy):\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h(x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h(x\n",
        "(i)\n",
        " ))]"
      ],
      "metadata": {
        "id": "bQEN6EKdoazA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "Adds a penalty to the cost function to prevent overfitting.\n",
        "\n",
        "Helps simplify the model by shrinking coefficients.\n",
        "\n",
        "6. Difference between Lasso, Ridge, and Elastic Net Regression:\n",
        "Ridge (L2): Shrinks coefficients, no elimination.\n",
        "\n",
        "Lasso (L1): Can shrink some coefficients to zero (feature selection).\n",
        "\n",
        "Elastic Net: Combines both L1 and L2.\n",
        "\n",
        "7. When should we use Elastic Net?\n",
        "When features are correlated and sparse (some irrelevant features).\n",
        "\n",
        "Balances feature selection (L1) and regularization (L2).\n",
        "\n",
        "8. Impact of regularization parameter (λ):\n",
        "High λ: Stronger regularization → simpler model, risk of underfitting.\n",
        "\n",
        "Low λ: Less regularization → complex model, risk of overfitting.\n",
        "\n",
        "9. Key assumptions of Logistic Regression:\n",
        "No multicollinearity.\n",
        "\n",
        "Large sample size.\n",
        "\n",
        "Linearity between independent variables and log-odds.\n",
        "\n",
        "10. Alternatives to Logistic Regression:\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Support Vector Machines\n",
        "\n",
        "K-Nearest Neighbors\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "11. Classification Evaluation Metrics:\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "12. Class imbalance impact:\n",
        "Skews accuracy.\n",
        "\n",
        "Model may favor majority class.\n",
        "\n",
        "Use metrics like F1-score or AUC, or apply resampling techniques.\n",
        "\n",
        "13. What is Hyperparameter Tuning?\n",
        "Optimizing parameters like C (inverse of λ), solver, penalty type using grid search, random search, etc.\n",
        "\n",
        "14. Different solvers in Logistic Regression:\n",
        "liblinear: Good for small datasets, supports L1.\n",
        "\n",
        "lbfgs: Efficient for large datasets, L2 only.\n",
        "\n",
        "saga: Supports both L1 and L2, works well on large datasets.\n",
        "\n",
        "Choose based on dataset size and regularization type.\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "One-vs-Rest (OvR)\n",
        "\n",
        "Multinomial Logistic Regression (Softmax)\n",
        "\n",
        "16. Advantages and Disadvantages of Logistic Regression:\n",
        "✅ Advantages:\n",
        "\n",
        "Simple, interpretable\n",
        "\n",
        "Fast and efficient\n",
        "\n",
        "Works well for linearly separable data\n",
        "\n",
        "❌ Disadvantages:\n",
        "\n",
        "Not effective for complex relationships\n",
        "\n",
        "Sensitive to outliers and multicollinearity\n",
        "\n",
        "17. Use cases of Logistic Regression:\n",
        "Email spam detection\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "18. Difference between Softmax and Logistic Regression:\n",
        "Logistic: Binary classification\n",
        "\n",
        "Softmax: Multiclass classification; outputs probabilities that sum to 1.\n",
        "\n",
        "19. OvR vs Softmax for Multiclass Classification:\n",
        "OvR: Train one classifier per class\n",
        "\n",
        "Softmax: One model, jointly considers all classes\n",
        "\n",
        "Use Softmax for better performance on balanced and multiclass problems.\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "Each coefficient represents the log-odds change in the outcome for a one-unit increase in the predictor, holding others constant.\n",
        "Exponentiating gives the odds ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "5P8izRd8ohFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical\n",
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracyC\n",
        "'2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracyC\n",
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C\n",
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'C\n",
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracyC\n",
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracyC\n",
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracyM\n",
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n",
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classificationM\n",
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-ScoreM\n",
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performanceM\n",
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performanceM\n",
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scalingM\n",
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM\n",
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracyM\n",
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficientsM\n",
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "ScoreM\n",
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification\n",
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracyM\n",
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)M\n",
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scalingM\n",
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validationM\n",
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "y-m4usYMo2N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_auc_score, cohen_kappa_score, matthews_corrcoef, ConfusionMatrixDisplay, PrecisionRecallDisplay\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Load a dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 1. Train/Test Split and Basic Logistic Regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"1. Accuracy:\", model.score(X_test, y_test))\n",
        "\n",
        "# 2. L1 Regularization\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model_l1.fit(X_train, y_train)\n",
        "print(\"2. L1 Accuracy:\", model_l1.score(X_test, y_test))\n",
        "\n",
        "# 3. L2 Regularization\n",
        "model_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "model_l2.fit(X_train, y_train)\n",
        "print(\"3. L2 Accuracy:\", model_l2.score(X_test, y_test))\n",
        "print(\"   Coefficients:\", model_l2.coef_)\n",
        "\n",
        "# 4. Elastic Net\n",
        "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model_elastic.fit(X_train, y_train)\n",
        "print(\"4. Elastic Net Accuracy:\", model_elastic.score(X_test, y_test))\n",
        "\n",
        "# 5. Multiclass OvR\n",
        "model_ovr = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "print(\"5. OvR Accuracy:\", model_ovr.score(X_test, y_test))\n",
        "\n",
        "# 6. GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "gs = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
        "gs.fit(X_train, y_train)\n",
        "print(\"6. Best Params:\", gs.best_params_)\n",
        "print(\"   Accuracy:\", gs.score(X_test, y_test))\n",
        "\n",
        "# 7. Stratified K-Fold CV\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=skf)\n",
        "print(\"7. Stratified K-Fold Accuracy:\", scores.mean())\n",
        "\n",
        "# 8. Load from CSV (Simulated using Iris DataFrame)\n",
        "pd.DataFrame(X, columns=iris.feature_names).to_csv(\"iris.csv\", index=False)\n",
        "data = pd.read_csv(\"iris.csv\")\n",
        "X_csv = data.values\n",
        "y_csv = y\n",
        "model_csv = LogisticRegression(max_iter=200)\n",
        "model_csv.fit(X_csv, y_csv)\n",
        "print(\"8. CSV Accuracy:\", model_csv.score(X_csv, y_csv))\n",
        "\n",
        "# 9. RandomizedSearchCV\n",
        "param_dist = {'C': np.logspace(-3, 3, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "rs = RandomizedSearchCV(LogisticRegression(max_iter=200), param_dist, n_iter=10, cv=5, random_state=42)\n",
        "rs.fit(X_train, y_train)\n",
        "print(\"9. Randomized Best Params:\", rs.best_params_)\n",
        "print(\"   Accuracy:\", rs.score(X_test, y_test))\n",
        "\n",
        "# 10. One-vs-One Multiclass\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "print(\"10. OvO Accuracy:\", ovo_model.score(X_test, y_test))\n",
        "\n",
        "# 11. Confusion Matrix\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.title(\"11. Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 12. Precision, Recall, F1-Score\n",
        "print(\"12. Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 13. Class Weights for Imbalanced Data\n",
        "model_weighted = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "print(\"13. Balanced Accuracy:\", model_weighted.score(X_test, y_test))\n",
        "\n",
        "# 14. Titanic Dataset Example\n",
        "# Simulate using breast cancer dataset\n",
        "bc = load_breast_cancer()\n",
        "X_bc, y_bc = bc.data, bc.target\n",
        "X_bc = pd.DataFrame(X_bc).fillna(X_bc.mean())\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.2)\n",
        "model_bc = LogisticRegression(max_iter=200)\n",
        "model_bc.fit(X_train_bc, y_train_bc)\n",
        "print(\"14. Titanic Example Accuracy:\", model_bc.score(X_test_bc, y_test_bc))\n",
        "\n",
        "# 15. Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_scaled, y)\n",
        "print(\"15. Accuracy with Scaling:\", model_scaled.score(X_scaled, y))\n",
        "print(\"    Accuracy without Scaling:\", model.score(X, y))\n",
        "\n",
        "# 16. ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test_bc, model_bc.predict_proba(X_test_bc)[:, 1])\n",
        "print(\"16. ROC-AUC:\", roc_auc)\n",
        "\n",
        "# 17. Custom Learning Rate (C=0.5)\n",
        "model_custom_c = LogisticRegression(C=0.5, max_iter=200)\n",
        "model_custom_c.fit(X_train, y_train)\n",
        "print(\"17. Custom C Accuracy:\", model_custom_c.score(X_test, y_test))\n",
        "\n",
        "# 18. Important Features\n",
        "print(\"18. Feature Importance:\", model.coef_)\n",
        "\n",
        "# 19. Cohen's Kappa\n",
        "print(\"19. Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
        "\n",
        "# 20. Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test_bc, model_bc.predict_proba(X_test_bc)[:, 1])\n",
        "PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
        "plt.title(\"20. Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\n",
        "# 21. Compare Solvers\n",
        "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
        "    model_solver = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model_solver.fit(X_train, y_train)\n",
        "    print(f\"21. Solver {solver} Accuracy:\", model_solver.score(X_test, y_test))\n",
        "\n",
        "# 22. Matthews Correlation Coefficient\n",
        "print(\"22. MCC:\", matthews_corrcoef(y_test, y_pred))\n",
        "\n",
        "# 23. Raw vs Standardized\n",
        "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X, y, test_size=0.2)\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_raw_train, y_raw_train)\n",
        "acc_raw = model_raw.score(X_raw_test, y_raw_test)\n",
        "scaler = StandardScaler()\n",
        "X_std_train = scaler.fit_transform(X_raw_train)\n",
        "X_std_test = scaler.transform(X_raw_test)\n",
        "model_std = LogisticRegression(max_iter=200)\n",
        "model_std.fit(X_std_train, y_raw_train)\n",
        "acc_std = model_std.score(X_std_test, y_raw_test)\n",
        "print(\"23. Accuracy Raw:\", acc_raw)\n",
        "print(\"    Accuracy Standardized:\", acc_std)\n",
        "\n",
        "# 24. Cross-validated C\n",
        "param_grid = {'C': np.logspace(-4, 4, 10)}\n",
        "gs_c = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
        "gs_c.fit(X_train, y_train)\n",
        "print(\"24. Best C:\", gs_c.best_params_)\n",
        "\n",
        "# 25. Save and Load Model\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "loaded_model = joblib.load(\"logistic_model.pkl\")\n",
        "y_loaded_pred = loaded_model.predict(X_test)\n",
        "print(\"25. Loaded Model Accuracy:\", accuracy_score(y_test, y_loaded_pred))\n"
      ],
      "metadata": {
        "id": "kaYQw2QSsFTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}